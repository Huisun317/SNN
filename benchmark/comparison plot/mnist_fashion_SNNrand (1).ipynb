{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "64051f42",
      "metadata": {
        "id": "64051f42"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import logging\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from scipy.stats import multivariate_normal as normal\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "sThDQwu9_MY6"
      },
      "id": "sThDQwu9_MY6",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6d9d4a68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d9d4a68",
        "outputId": "978eacf2-4818-4ca8-ef64-6e84efae03dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3d24149c10>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a8048c60",
      "metadata": {
        "id": "a8048c60"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "data_type=torch.float32\n",
        "MOMENTUM = 0.99\n",
        "EPSILON = 1e-6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEXLxVU3HSjm",
        "outputId": "a5265ffb-2980-4d85-f08c-ad46fed4e90d"
      },
      "id": "VEXLxVU3HSjm",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "043871cf",
      "metadata": {
        "id": "043871cf"
      },
      "source": [
        "# Handling the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "    batch_size = 500\n",
        "\n",
        "    totalT=2.0;\n",
        "\n",
        "    n_layer=Ntime=4;\n",
        "\n",
        "    sqrt_deltaT=np.sqrt(totalT/Ntime);\n",
        "\n",
        "    logging_frequency = 100\n",
        "    verbose = True\n",
        "\n",
        "    input_chanel=1\n",
        "    output_chanel_pj1=32\n",
        "    output_chanel_pj2=16\n",
        "\n",
        "    unflatten_shape=output_chanel_pj2*7*7\n",
        "\n",
        "def get_config(name):\n",
        "    try:\n",
        "        return globals()[name]\n",
        "    except KeyError:\n",
        "        raise KeyError(\"config not defined.\")\n",
        "\n",
        "cfg=get_config('Config')"
      ],
      "metadata": {
        "id": "39nRMtKvCzHs"
      },
      "id": "39nRMtKvCzHs",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e9b3546f",
      "metadata": {
        "scrolled": true,
        "id": "e9b3546f"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "batch_size_train=cfg.batch_size\n",
        "batch_size_test=cfg.batch_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "            datasets.FashionMNIST('/files/', train=True, download=True,\n",
        "                transform=transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])), batch_size=cfg.batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "            datasets.FashionMNIST('/files/', train=False, download=True,\n",
        "                transform=transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])), batch_size=cfg.batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "gM8FBI_d4lLz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f648cc-a8e5-4880-8ca8-c21120ce7202"
      },
      "id": "gM8FBI_d4lLz",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /files/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 8370954.84it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /files/FashionMNIST/raw/train-images-idx3-ubyte.gz to /files/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /files/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 141518.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /files/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /files/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /files/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 2583484.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /files/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /files/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /files/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 8544628.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /files/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /files/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)"
      ],
      "metadata": {
        "id": "n80EcIib4lOl"
      },
      "id": "n80EcIib4lOl",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bceb0197",
      "metadata": {
        "id": "bceb0197"
      },
      "source": [
        "We have stored both the training and the validation datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70444346",
      "metadata": {
        "id": "70444346"
      },
      "source": [
        "Defining the dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78047303",
      "metadata": {
        "id": "78047303"
      },
      "source": [
        "## Defining the configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "be29e470",
      "metadata": {
        "id": "be29e470"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c411bccd",
      "metadata": {
        "id": "c411bccd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "407bcc53",
      "metadata": {
        "id": "407bcc53"
      },
      "source": [
        "# Constructing a dense net"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fefbf48",
      "metadata": {
        "id": "5fefbf48"
      },
      "source": [
        "## Building the building block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "96a33786",
      "metadata": {
        "id": "96a33786"
      },
      "outputs": [],
      "source": [
        "class ProjBlock(nn.Module):\n",
        "    def __init__(self,input_chanel,output_chanel):\n",
        "        super(ProjBlock,self).__init__()\n",
        "        self.input_chanel=input_chanel\n",
        "        self.output_chanel=output_chanel\n",
        "\n",
        "        self.conv1=nn.Conv2d(input_chanel,output_chanel,kernel_size=3,padding=1)\n",
        "        self.act1=nn.Tanh()\n",
        "        self.pool1=nn.MaxPool2d(2)\n",
        "\n",
        "      #  self.conv2=nn.Conv2d(2*output_chanel,output_chanel,kernel_size=3,padding=1)\n",
        "      #  self.act1=nn.Tanh()\n",
        "      #  self.pool1=nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "      #  out = self.pool2(self.act2(self.conv2(x)))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3f600188",
      "metadata": {
        "id": "3f600188"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self,num_chanel):\n",
        "        super(BasicBlock,self).__init__()\n",
        "        self.input_chanel=num_chanel\n",
        "        self.output_chanel=num_chanel\n",
        "\n",
        "        self.conv=nn.Conv2d(self.input_chanel,self.output_chanel,kernel_size=3,padding=1)\n",
        "        self.act=nn.Tanh()\n",
        "        ## there should not be any MaxPooling layer in the inbetween set\n",
        "\n",
        "    def forward(self,x):\n",
        "        out=self.act(self.conv(x))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "41ff83cf",
      "metadata": {
        "id": "41ff83cf"
      },
      "outputs": [],
      "source": [
        "# One is responsible for figuring out the unflatten shape\n",
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self,unflatten_shape):\n",
        "        super(FullyConnected,self).__init__()\n",
        "        self.unflatten_shape=unflatten_shape\n",
        "        self.fc1=nn.Linear(unflatten_shape,32)\n",
        "        self.ac1=nn.Tanh()\n",
        "        self.fc2=nn.Linear(32,10)\n",
        "        # Let's only tell the airplane from a bird\n",
        "\n",
        "    def forward(self,x):\n",
        "        inputx=x.view(-1, self.unflatten_shape)\n",
        "        out=self.fc2(self.ac1(self.fc1(inputx)))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30ec1467",
      "metadata": {
        "id": "30ec1467"
      },
      "source": [
        "## Stacking up the blocks"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XGZjVG22QYRe"
      },
      "id": "XGZjVG22QYRe",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal.rvs(size=[2,2],random_state=12345)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrQQIQxjQwQf",
        "outputId": "2769326e-5df8-40c8-967d-c0199043a286"
      },
      "id": "yrQQIQxjQwQf",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.20470766,  0.47894334],\n",
              "       [-0.51943872, -0.5557303 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normal.rvs(size=[2,2],random_state=12345)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm2CapdrQwS5",
        "outputId": "6af3f1e9-7289-44e4-ecec-bb0969602cff"
      },
      "id": "Pm2CapdrQwS5",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.20470766,  0.47894334],\n",
              "       [-0.51943872, -0.5557303 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "0eebf8c6",
      "metadata": {
        "id": "0eebf8c6"
      },
      "outputs": [],
      "source": [
        "loss_fn=nn.CrossEntropyLoss()\n",
        "class ForwardModel(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super(ForwardModel,self).__init__()\n",
        "\n",
        "        self.config=config\n",
        "        self.batch_size=self.config.batch_size\n",
        "        self.Ntime=self.config.Ntime\n",
        "        self.sqrt_deltaT=self.config.sqrt_deltaT;\n",
        "        self.n_layer=self.config.n_layer\n",
        "        self.delta=self.config.totalT/self.Ntime;\n",
        "\n",
        "        self.mList=nn.ModuleList([ProjBlock(self.config.input_chanel,self.config.output_chanel_pj1),\n",
        "                                  ProjBlock(self.config.output_chanel_pj1,self.config.output_chanel_pj2),\n",
        "                                  BasicBlock(self.config.output_chanel_pj2),\n",
        "                                  BasicBlock(self.config.output_chanel_pj2),\n",
        "                                  BasicBlock(self.config.output_chanel_pj2),\n",
        "                                  BasicBlock(self.config.output_chanel_pj2),\n",
        "                                  FullyConnected(self.config.unflatten_shape)\n",
        "        ])\n",
        "\n",
        "        self.mList_diff=nn.ModuleList([\n",
        "                                  BasicBlock(self.config.output_chanel_pj2),\n",
        "                                  BasicBlock(self.config.output_chanel_pj2),\n",
        "                                  BasicBlock(self.config.output_chanel_pj2),\n",
        "                                  BasicBlock(self.config.output_chanel_pj2)\n",
        "        ])\n",
        "\n",
        "        self.sigma=0.2\n",
        "\n",
        "    def forwardX(self,x):# here x is the batch collection of images\n",
        "\n",
        "        # Constructing the noises\n",
        "        # The number 8 is determined from the number of max-pooling size, kernels & paddings etc.\n",
        "        xMat=[]\n",
        "        wMat=self.sigma*torch.FloatTensor(normal.rvs(size=[self.batch_size,        ### The batch_size for each different data point.\n",
        "                                     self.config.output_chanel_pj2,7,7,\n",
        "                                     self.Ntime]) * self.sqrt_deltaT).to(device)\n",
        "        x0=torch.clone(x).to(device);\n",
        "        xMat.append(x0);\n",
        "\n",
        "        x_pj1=self.mList[0](x0);\n",
        "        xMat.append(x_pj1.to(device));\n",
        "        x_input=self.mList[1](x_pj1)\n",
        "        xMat.append(x_input.to(device));\n",
        "\n",
        "        for i in range(self.Ntime):\n",
        "            # i + 2 because we already have two layers before\n",
        "            xtemp=xMat[i+2]+self.mList[i+2](xMat[i+2])*self.delta +  self.mList_diff[i](xMat[i+2]) *wMat[:,:,:,:,i] ## torch.sigmoid\n",
        "            xMat.append(xtemp.to(device))\n",
        "\n",
        "        x_terminal=self.mList[-1](xMat[-1])\n",
        "        xMat.append(x_terminal.to(device))\n",
        "\n",
        "        return xMat, wMat\n",
        "\n",
        "        # The input of the target must be a tensor not a list\n",
        "\n",
        "\n",
        "    def backwardYZ(self,xMat,wMat,target):\n",
        "        yMat=[];\n",
        "        zMat=[];\n",
        "\n",
        "\n",
        "        L=len(xMat)\n",
        "        x_terminal=xMat[-1].to(device)\n",
        "\n",
        "        loss_val=loss_fn(x_terminal,target.to(device))\n",
        "        loss_val.to(device);\n",
        "\n",
        "        y_terminal=torch.autograd.grad(outputs=[loss_val], inputs=[x_terminal], grad_outputs=torch.ones_like(loss_val), allow_unused=True,\n",
        "                                 retain_graph=True, create_graph=True)[0]\n",
        "        #Here y_terminal has dim batch_size x output_size (2 x 2)\n",
        "        yMat.append(y_terminal.to(device));\n",
        "        xtemp=xMat[L-2].to(device) # 3\n",
        "\n",
        "        ## Finding Y[T-1]\n",
        "        hami=torch.sum(y_terminal.detach()*self.mList[-1](xtemp),dim=1,keepdim=True) # keep dim=1 is correct\n",
        "        hami=hami.view(-1,1);hami.to(device)\n",
        "\n",
        "        hami_x=torch.autograd.grad(outputs=[hami], inputs=[xtemp], grad_outputs=torch.ones_like(hami),allow_unused=True,\n",
        "                                 retain_graph=True, create_graph=True)[0]\n",
        "\n",
        "        yMat.append(hami_x.to(device))\n",
        "\n",
        "        for i in range(self.Ntime-1,-1,-1):\n",
        "######### for Z ##\n",
        "            ztemp=yMat[-1]*wMat[:,:,:,:,i]/self.sqrt_deltaT\n",
        "            zMat.append(ztemp)\n",
        "\n",
        "            X=xMat[i+2].to(device);\n",
        "            hami=torch.sum(yMat[-1].detach()*self.mList[i+2](X) + ztemp.detach()*self.mList_diff[i](X),dim=(1,2,3))\n",
        "            hami=hami.view(-1,1); hami.to(device);\n",
        "\n",
        "            hami_x=torch.autograd.grad(outputs=[hami], inputs=[X], grad_outputs=torch.ones_like(hami),allow_unused=True,\n",
        "                                 retain_graph=True, create_graph=True)[0]\n",
        "            ytemp=yMat[-1]+hami_x*self.delta\n",
        "\n",
        "            yMat.append(ytemp.to(device))\n",
        "\n",
        "    ### Second projection layer\n",
        "        X=xMat[1].to(device);\n",
        "       # X.requires_grad\n",
        "        hami=torch.sum(yMat[-1].detach()*self.mList[1](X),dim=(1,2,3))\n",
        "        hami=hami.view(-1,1); hami.to(device);\n",
        "\n",
        "        ytemp=torch.autograd.grad(outputs=[hami], inputs=[X], grad_outputs=torch.ones_like(hami),allow_unused=True,\n",
        "                                 retain_graph=True, create_graph=True)[0]\n",
        "        yMat.append(ytemp.to(device))\n",
        "\n",
        "        X=xMat[0].to(device);\n",
        "        X.requires_grad=True\n",
        "        hami=torch.sum(yMat[-1].detach()*self.mList[0](X),dim=(1,2,3))\n",
        "        hami=hami.view(-1,1); hami.to(device)\n",
        "\n",
        "        ytemp=torch.autograd.grad(outputs=[hami], inputs=[X], grad_outputs=torch.ones_like(hami),allow_unused=True,\n",
        "                                 retain_graph=True, create_graph=True)[0]\n",
        "        yMat.append(ytemp.to(device))\n",
        "\n",
        "        return yMat,zMat  #yMat the order is reversed\n",
        "\n",
        "\n",
        "    def HamCompute(self,xMat,yMat,zMat):\n",
        "        totalham=0.0\n",
        "       # l2_norm=sum(p.pow(2.0).sum() for p in self.parameters() )\n",
        "        for i in range(self.Ntime+3):\n",
        "            ham_temp=torch.sum(yMat[self.Ntime+2-i].detach()*self.mList[i](xMat[i].detach()) )  #inside the bracket =  +\\small_value * self.batch_size *self.mList[i]*self.mList[i] (No, this doesn't contain batchsize)\n",
        "            totalham+=ham_temp\n",
        "        for i in range(self.Ntime-1, -1 , -1):\n",
        "          ham_temp=torch.sum(zMat[i].detach()*self.mList_diff[i](xMat[i+2].detach()))\n",
        "          totalham+=ham_temp\n",
        "        #totalham+=l2_norm*0.001\n",
        "        return totalham/self.batch_size/(self.Ntime+3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_accuracy(train_loader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for imgs, labels in train_loader:\n",
        "          outputs = net.forwardX(imgs)\n",
        "          _, predicted = torch.max(outputs[0][-1], dim=1)\n",
        "          total += labels.shape[0]\n",
        "          correct += int((predicted == labels.to(device)).sum())\n",
        "  res=correct/total\n",
        "\n",
        "  return res\n",
        "\n",
        "def test_accuracy(val_loader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for imgs, labels in val_loader:\n",
        "          outputs =net.forwardX(imgs)\n",
        "          _, predicted = torch.max(outputs[0][-1], dim=1)\n",
        "          total += labels.shape[0]\n",
        "          correct += int((predicted == labels.to(device)).sum())\n",
        "  res=correct/total\n",
        "  return res"
      ],
      "metadata": {
        "id": "9RylV-3v1r1l"
      },
      "id": "9RylV-3v1r1l",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch=10\n",
        "\n",
        "net=ForwardModel(cfg)\n",
        "net.to(device)\n",
        "\n",
        "optimizer=optim.Adam(net.parameters(), lr=1.5e-3)#it could be a bad idea to add weight decay\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1000,2500,4000], gamma=0.2)\n",
        "\n",
        "Loss_vec=[]\n",
        "training_accuracy=[]\n",
        "testing_accuracy=[]\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    for imgs, labels in train_loader:\n",
        "\n",
        "\n",
        "        xmat,wmat=net.forwardX(imgs);\n",
        "        ymat,zmat=net.backwardYZ(xmat,wmat.to(device),labels)\n",
        "        loss_temp=net.HamCompute(xmat,ymat,zmat)\n",
        "        loss_temp.to(device)\n",
        "\n",
        "        optimizer.zero_grad();\n",
        "        loss_temp.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch %1 ==0:\n",
        "        loss_val=loss_fn(xmat[-1].to(device),labels.to(device))\n",
        "       # ham_loss=net.HamCompute(xmat,ymat)\n",
        "       # print(ham_loss.cpu().detach().numpy(), loss_val.cpu().detach().numpy())\n",
        "        loss_val_np=loss_val.cpu().detach().numpy()\n",
        "        print(epoch, loss_val_np)\n",
        "        Loss_vec.append(loss_val_np)\n",
        "\n",
        "    #if epoch %10 ==0:\n",
        "        test_temp=test_accuracy(test_loader)\n",
        "        testing_accuracy.append(test_temp)\n",
        "        print(test_temp)\n",
        "\n",
        "        train_temp=train_accuracy(train_loader)\n",
        "        training_accuracy.append(train_temp)\n",
        "        print(train_temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQCOsVqdE1Tr",
        "outputId": "8d144748-cc26-415a-f11e-7bbcfe051fac"
      },
      "id": "yQCOsVqdE1Tr",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.47832537\n",
            "0.8367\n",
            "0.8479166666666667\n",
            "1 0.36684072\n",
            "0.857\n",
            "0.86975\n",
            "2 0.288941\n",
            "0.8747\n",
            "0.88525\n",
            "3 0.31472087\n",
            "0.8846\n",
            "0.8976333333333333\n",
            "4 0.30505952\n",
            "0.8888\n",
            "0.9027333333333334\n",
            "5 0.2971817\n",
            "0.8964\n",
            "0.9132666666666667\n",
            "6 0.23081361\n",
            "0.8983\n",
            "0.9168\n",
            "7 0.24328998\n",
            "0.8979\n",
            "0.91845\n",
            "8 0.23173404\n",
            "0.8997\n",
            "0.9183666666666667\n",
            "9 0.29068804\n",
            "0.9025\n",
            "0.9263666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "SNfMfoSEh2uC"
      },
      "id": "SNfMfoSEh2uC",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(training_accuracy).to_csv(\"02_snnTrain.csv\")\n",
        "pd.DataFrame(testing_accuracy).to_csv(\"02_snnTest.csv\")"
      ],
      "metadata": {
        "id": "XdWQoHvTE1WH"
      },
      "id": "XdWQoHvTE1WH",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZOeADJH9E1Ya"
      },
      "id": "ZOeADJH9E1Ya",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch=20\n",
        "\n",
        "net=ForwardModel(cfg)\n",
        "net.to(device)\n",
        "\n",
        "optimizer=optim.Adam(net.parameters(), lr=1.5e-3)#it could be a bad idea to add weight decay\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1000,2500,4000], gamma=0.2)\n",
        "\n",
        "Loss_vec=[]\n",
        "training_accuracy=[]\n",
        "testing_accuracy=[]\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    for imgs, labels in train_loader:\n",
        "\n",
        "\n",
        "        xmat,wmat=net.forwardX(imgs);\n",
        "        ymat=net.backwardYZ(xmat,wmat.to(device),labels)\n",
        "        loss_temp=net.HamCompute(xmat,ymat)\n",
        "        loss_temp.to(device)\n",
        "\n",
        "        optimizer.zero_grad();\n",
        "        loss_temp.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch %1 ==0:\n",
        "        loss_val=loss_fn(xmat[-1].to(device),labels.to(device))\n",
        "       # ham_loss=net.HamCompute(xmat,ymat)\n",
        "       # print(ham_loss.cpu().detach().numpy(), loss_val.cpu().detach().numpy())\n",
        "        loss_val_np=loss_val.cpu().detach().numpy()\n",
        "        print(epoch, loss_val_np)\n",
        "        Loss_vec.append(loss_val_np)\n",
        "\n",
        "    #if epoch %10 ==0:\n",
        "        test_temp=test_accuracy(test_loader)\n",
        "        testing_accuracy.append(test_temp)\n",
        "        print(test_temp)\n",
        "\n",
        "        train_temp=train_accuracy(train_loader)\n",
        "        training_accuracy.append(train_temp)\n",
        "        print(train_temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rzfS1xf9V-V",
        "outputId": "3cc15f96-90ef-4c36-ac27-0f9391c65cbd"
      },
      "id": "-rzfS1xf9V-V",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.4600175\n",
            "0.8342\n",
            "0.84065\n",
            "1 0.3930344\n",
            "0.8601\n",
            "0.8712166666666666\n",
            "2 0.3804343\n",
            "0.8762\n",
            "0.8894166666666666\n",
            "3 0.31233355\n",
            "0.8764\n",
            "0.8934\n",
            "4 0.33462223\n",
            "0.89\n",
            "0.9055\n",
            "5 0.2883765\n",
            "0.8934\n",
            "0.9103166666666667\n",
            "6 0.24727301\n",
            "0.8978\n",
            "0.9158166666666666\n",
            "7 0.24122407\n",
            "0.8946\n",
            "0.9167\n",
            "8 0.22976273\n",
            "0.9036\n",
            "0.9237166666666666\n",
            "9 0.25150692\n",
            "0.908\n",
            "0.9311\n",
            "10 0.23556092\n",
            "0.9021\n",
            "0.9269833333333334\n",
            "11 0.20559546\n",
            "0.9096\n",
            "0.9346166666666667\n",
            "12 0.20727155\n",
            "0.9073\n",
            "0.9358333333333333\n",
            "13 0.16154906\n",
            "0.9059\n",
            "0.936\n",
            "14 0.17529136\n",
            "0.9134\n",
            "0.9409333333333333\n",
            "15 0.15335551\n",
            "0.9096\n",
            "0.944\n",
            "16 0.19574562\n",
            "0.9133\n",
            "0.948\n",
            "17 0.14522822\n",
            "0.9152\n",
            "0.9506\n",
            "18 0.12336541\n",
            "0.911\n",
            "0.9519\n",
            "19 0.105183505\n",
            "0.9147\n",
            "0.9537166666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ow3JxQNs9WAf"
      },
      "id": "ow3JxQNs9WAf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dsh5jtjm9WC0"
      },
      "id": "Dsh5jtjm9WC0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LFB9Cr2vBNML"
      },
      "id": "LFB9Cr2vBNML",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "awQUAAoHBNO9"
      },
      "id": "awQUAAoHBNO9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XIOxQ8bABNRp"
      },
      "id": "XIOxQ8bABNRp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "332f3bfb",
      "metadata": {
        "id": "332f3bfb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1319adca",
      "metadata": {
        "id": "1319adca"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1be28a50",
      "metadata": {
        "id": "1be28a50"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}