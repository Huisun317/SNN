{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11de4f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time \n",
    "import logging\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from scipy.stats import multivariate_normal as normal\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62514b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x24918744cb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "586642ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dim=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca8674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b81d41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "data_type=torch.float32\n",
    "MOMENTUM = 0.99\n",
    "EPSILON = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb638d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    batch_size = 500\n",
    "    \n",
    "    totalT=2.0;\n",
    "    \n",
    "    n_layer=Ntime=4; \n",
    "    \n",
    "    sqrt_deltaT=np.sqrt(totalT/Ntime); \n",
    "\n",
    "    logging_frequency = 100\n",
    "    verbose = True\n",
    "   \n",
    "    input_chanel=1\n",
    "    output_chanel_pj1=32\n",
    "    output_chanel_pj2=16 \n",
    "    \n",
    "    unflatten_shape=output_chanel_pj2*7*7\n",
    "    \n",
    "def get_config(name):\n",
    "    try:\n",
    "        return globals()[name]\n",
    "    except KeyError:\n",
    "        raise KeyError(\"config not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea1b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg=get_config('Config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cbcb645",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train=cfg.batch_size\n",
    "batch_size_test=cfg.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c626bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa5add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91dace",
   "metadata": {},
   "source": [
    "## The building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2a4ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjBlock(nn.Module):\n",
    "    def __init__(self,input_chanel,output_chanel):\n",
    "        super(ProjBlock,self).__init__()\n",
    "        self.input_chanel=input_chanel\n",
    "        self.output_chanel=output_chanel\n",
    "        \n",
    "        self.conv1=nn.Conv2d(input_chanel,output_chanel,kernel_size=3,padding=1) \n",
    "        self.act1=nn.Tanh()\n",
    "        self.pool1=nn.MaxPool2d(2)\n",
    "        \n",
    "      #  self.conv2=nn.Conv2d(2*output_chanel,output_chanel,kernel_size=3,padding=1) \n",
    "      #  self.act1=nn.Tanh()\n",
    "      #  self.pool1=nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "      #  out = self.pool2(self.act2(self.conv2(x)))\n",
    "        return out\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self,num_chanel):\n",
    "        super(BasicBlock,self).__init__()\n",
    "        self.input_chanel=num_chanel\n",
    "        self.output_chanel=num_chanel\n",
    "        \n",
    "        self.conv=nn.Conv2d(self.input_chanel,self.output_chanel,kernel_size=3,padding=1)\n",
    "        self.act=nn.Tanh()\n",
    "        ## there should not be any MaxPooling layer in the inbetween set\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.act(self.conv(x))\n",
    "        return out\n",
    "\n",
    "# One is responsible for figuring out the unflatten shape\n",
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self,unflatten_shape): \n",
    "        super(FullyConnected,self).__init__()\n",
    "        self.unflatten_shape=unflatten_shape\n",
    "        self.fc1=nn.Linear(unflatten_shape,32)\n",
    "        self.ac1=nn.Tanh()\n",
    "        self.fc2=nn.Linear(32,10) \n",
    "        # Let's only tell the airplane from a bird\n",
    "    \n",
    "    def forward(self,x):\n",
    "        inputx=x.view(-1, self.unflatten_shape)\n",
    "        out=self.fc2(self.ac1(self.fc1(inputx)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a393e0",
   "metadata": {},
   "source": [
    "### Structure of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4c5b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()\n",
    "class ForwardModel(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(ForwardModel,self).__init__()\n",
    "        \n",
    "        self.config=config\n",
    "        self.batch_size=self.config.batch_size\n",
    "        self.Ntime=self.config.Ntime\n",
    "        self.sqrt_deltaT=self.config.sqrt_deltaT;\n",
    "        self.n_layer=self.config.n_layer\n",
    "        self.delta=self.config.totalT/self.Ntime;\n",
    "        \n",
    "        ## The structure is merely a stack-up of the convolutional blocks\n",
    "        self.mList=nn.ModuleList([ProjBlock(self.config.input_chanel,self.config.output_chanel_pj1),\n",
    "                                  ProjBlock(self.config.output_chanel_pj1,self.config.output_chanel_pj2),\n",
    "                                  BasicBlock(self.config.output_chanel_pj2),\n",
    "                                  BasicBlock(self.config.output_chanel_pj2),\n",
    "                                  BasicBlock(self.config.output_chanel_pj2),\n",
    "                                  BasicBlock(self.config.output_chanel_pj2),\n",
    "                                  FullyConnected(self.config.unflatten_shape)                              \n",
    "        ])\n",
    "        \n",
    "        self.sigma=0.25\n",
    "        \n",
    "    def forwardX(self,x):# here x is the batch collection of images\n",
    "        \n",
    "        # Constructing the noises\n",
    "        # The number 8 is determined from the number of max-pooling size, kernels & paddings etc. \n",
    "        xMat=[]\n",
    "        wMat=torch.FloatTensor(normal.rvs(size=[self.batch_size,\n",
    "                                     self.config.output_chanel_pj2,7,7,\n",
    "                                     self.Ntime]) * self.sqrt_deltaT).to(device)\n",
    "        x0=torch.clone(x).to(device); \n",
    "        xMat.append(x0); \n",
    "        \n",
    "        x_pj1=self.mList[0](x0); \n",
    "        xMat.append(x_pj1.to(device)); \n",
    "        x_input=self.mList[1](x_pj1)\n",
    "        xMat.append(x_input.to(device));\n",
    "        \n",
    "        for i in range(self.Ntime):\n",
    "            # i + 2 because we already have two layers before\n",
    "            xtemp=xMat[i+2]+self.mList[i+2](xMat[i+2])*self.delta +self.sigma*wMat[:,:,:,:,i] \n",
    "            xMat.append(xtemp.to(device))\n",
    "        \n",
    "        x_terminal=self.mList[-1](xMat[-1])\n",
    "        xMat.append(x_terminal.to(device))\n",
    "        \n",
    "        \n",
    "        return xMat, wMat\n",
    "        \n",
    "        # The input of the target must be a tensor not a list\n",
    "    def backwardYZ(self,xMat,wMat,target):\n",
    "        yMat=[];  \n",
    "        \n",
    "        L=len(xMat)\n",
    "        x_terminal=xMat[-1].to(device)\n",
    "        \n",
    "        loss_val=loss_fn(x_terminal,target.to(device))\n",
    "        loss_val.to(device); \n",
    "        \n",
    "        y_terminal=torch.autograd.grad(outputs=[loss_val], inputs=[x_terminal], grad_outputs=torch.ones_like(loss_val), allow_unused=True,\n",
    "                                 retain_graph=True, create_graph=True)[0]\n",
    "        #Here y_terminal has dim batch_size x output_size (2 x 2)\n",
    "        yMat.append(y_terminal.to(device)); \n",
    "        xtemp=xMat[L-2].to(device) # 3 \n",
    "        \n",
    "        ## Finding Y[T-1]\n",
    "        hami=torch.sum(y_terminal.detach()*self.mList[-1](xtemp),dim=1,keepdim=True) # keep dim=1 is correct\n",
    "        hami=hami.view(-1,1);hami.to(device)\n",
    "\n",
    "        hami_x=torch.autograd.grad(outputs=[hami], inputs=[xtemp], grad_outputs=torch.ones_like(hami),allow_unused=True,\n",
    "                                 retain_graph=True, create_graph=True)[0]\n",
    "    \n",
    "        yMat.append(hami_x.to(device))\n",
    "        \n",
    "        for i in range(self.Ntime-1,-1,-1):\n",
    "            X=xMat[i+2].to(device); \n",
    "            hami=torch.sum(yMat[-1].detach()*self.mList[i+2](X),dim=(1,2,3))\n",
    "            hami=hami.view(-1,1); hami.to(device); \n",
    "            \n",
    "            hami_x=torch.autograd.grad(outputs=[hami], inputs=[X], grad_outputs=torch.ones_like(hami),allow_unused=True,\n",
    "                                 retain_graph=True, create_graph=True)[0]\n",
    "            ytemp=yMat[-1]+hami_x*self.delta\n",
    "\n",
    "            yMat.append(ytemp.to(device))\n",
    "       \n",
    "    ### Second projection layer\n",
    "        X=xMat[1].to(device); \n",
    "       # X.requires_grad\n",
    "        hami=torch.sum(yMat[-1].detach()*self.mList[1](X),dim=(1,2,3))\n",
    "        hami=hami.view(-1,1); hami.to(device); \n",
    "            \n",
    "        ytemp=torch.autograd.grad(outputs=[hami], inputs=[X], grad_outputs=torch.ones_like(hami),allow_unused=True,\n",
    "                                 retain_graph=True, create_graph=True)[0]\n",
    "        yMat.append(ytemp.to(device))\n",
    "        \n",
    "        X=xMat[0].to(device); \n",
    "        X.requires_grad=True\n",
    "        hami=torch.sum(yMat[-1].detach()*self.mList[0](X),dim=(1,2,3))\n",
    "        hami=hami.view(-1,1); hami.to(device)\n",
    "            \n",
    "        ytemp=torch.autograd.grad(outputs=[hami], inputs=[X], grad_outputs=torch.ones_like(hami),allow_unused=True,\n",
    "                                 retain_graph=True, create_graph=True)[0]\n",
    "        yMat.append(ytemp.to(device))\n",
    "        \n",
    "        return yMat  #yMat the order is reversed \n",
    "    \n",
    "    def HamCompute(self,xMat,yMat):\n",
    "        totalham=0.0\n",
    "        \n",
    "        for i in range(self.Ntime+3):\n",
    "            ham_temp=torch.sum(yMat[self.Ntime+2-i].detach()*self.mList[i](xMat[i].detach()) )  #inside the bracket =  +\\small_value * self.batch_size *self.mList[i]*self.mList[i] (No, this doesn't contain batchsize)\n",
    "            totalham+=ham_temp\n",
    "        \n",
    "        return totalham/self.batch_size/(self.Ntime+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26f69326",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"data/SNN_mnist_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96d572ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=ForwardModel(cfg)\n",
    "net.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc9cbc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(pretrained_model, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00cb9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "#net.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3096b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM attack code\n",
    "# which also works in the batch case\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "#    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7422c",
   "metadata": {},
   "source": [
    "### Defining the test accuracy function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9fb4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_prediction(model,iters,imgs,labels):\n",
    "    temp=torch.zeros((cfg.batch_size,label_dim))\n",
    "    temp=temp.to(device)\n",
    "    for i in range(iters):\n",
    "        val,_=model.forwardX(imgs)\n",
    "        temp+=val[-1].to(device)\n",
    "    temp=temp/iters;\n",
    "    _,predicts=torch.max(temp,dim=1)\n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c564d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42b0933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "correct_0=0 \n",
    "total_0=0\n",
    "\n",
    "for imgs, labels in test_loader:\n",
    "    imgs, labels=imgs.to(device), labels.to(device)\n",
    "    imgs.requires_grad=True\n",
    " \n",
    "    \n",
    "    pred=avg_prediction(net,2,imgs,labels)\n",
    "    correct_0 += int((pred.to(device) == labels.to(device)).sum())\n",
    "    total_0 += imgs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ee3662a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9879"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_0/total_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b21e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=ForwardModel(cfg)\n",
    "net.to(device);\n",
    "net.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "correct_post=0\n",
    "total_post=0\n",
    "optimizer=optim.Adam(net.parameters(), lr=1.5e-3)\n",
    "\n",
    "attack_epsilon=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e87c360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels=imgs.to(device), labels.to(device)\n",
    "        imgs.requires_grad=True\n",
    "\n",
    "        ### Attacker starts to work ### \n",
    "        net.eval()\n",
    "        xm,wm=net.forwardX(imgs)\n",
    "\n",
    "\n",
    "        loss=loss_fn(xm[-1].to(device),labels)\n",
    "        _,pred=torch.max(xm[-1],dim=1)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        data_grad=imgs.grad.data # you can obtain the grad in this way??? \n",
    "        peturbed_data=fgsm_attack(imgs,attack_epsilon,data_grad)  \n",
    "\n",
    "        pred_attack=avg_prediction(net,2,peturbed_data,labels)\n",
    "\n",
    "\n",
    "        correct += int((pred_attack.to(device) == labels.to(device)).sum())\n",
    "        total += imgs.shape[0]\n",
    "\n",
    "        ### Self-correction made by the model ### \n",
    "        peturbed_data= peturbed_data.detach()\n",
    "        net.train() #Now we set the net to train mode\n",
    "\n",
    "        xm_post, wm_post =net.forwardX(peturbed_data)\n",
    "        ym_post=net.backwardYZ(xm_post,wm_post.to(device), labels)\n",
    "        loss_temp=net.HamCompute(xm_post,ym_post)\n",
    "        loss_temp.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_temp.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "       # output_trained,_ =net(peturbed_data)\n",
    "        pred_trained=avg_prediction(net,2,peturbed_data,labels)\n",
    "\n",
    "        correct_post += int((pred_trained.to(device) == labels.to(device)).sum())\n",
    "        total_post += imgs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29e2b7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.75798, 0.7669)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We point out that this step collects all the correct predictions (for all 5 epochs) \n",
    "### post attack divided by the total samples \n",
    "correct_post/total_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "542bd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "correct_natural=0 \n",
    "total_natural=0\n",
    "\n",
    "for imgs, labels in test_loader:\n",
    "    imgs, labels=imgs.to(device), labels.to(device)\n",
    "    imgs.requires_grad=True\n",
    " \n",
    "    \n",
    "    pred=avg_prediction(net,2,imgs,labels)\n",
    "    correct_natural += int((pred.to(device) == labels.to(device)).sum())\n",
    "    total_natural += imgs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9369780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.974"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The natural accuracy post attack\n",
    "correct_natural/total_natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbfcebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6b092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e79d9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580800bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53169f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd6a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c856b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca1fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895aeb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a8732e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e63f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
