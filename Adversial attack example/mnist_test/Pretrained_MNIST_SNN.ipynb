{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11de4f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time \n",
    "import logging\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from scipy.stats import multivariate_normal as normal\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62514b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21fb44c3b90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "586642ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dim=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca8674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b81d41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "data_type=torch.float32\n",
    "MOMENTUM = 0.99\n",
    "EPSILON = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb638d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    batch_size = 500\n",
    "    \n",
    "    totalT=2.0;\n",
    "    \n",
    "    n_layer=Ntime=4; \n",
    "    \n",
    "    sqrt_deltaT=np.sqrt(totalT/Ntime); \n",
    "\n",
    "    logging_frequency = 100\n",
    "    verbose = True\n",
    "   \n",
    "    input_chanel=1\n",
    "    output_chanel_pj1=32\n",
    "    output_chanel_pj2=16 \n",
    "    \n",
    "    unflatten_shape=output_chanel_pj2*7*7\n",
    "    \n",
    "def get_config(name):\n",
    "    try:\n",
    "        return globals()[name]\n",
    "    except KeyError:\n",
    "        raise KeyError(\"config not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea1b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg=get_config('Config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cbcb645",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train=cfg.batch_size\n",
    "batch_size_test=cfg.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c626bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa5add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91dace",
   "metadata": {},
   "source": [
    "## The building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2a4ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjBlock(nn.Module):\n",
    "    def __init__(self,input_chanel,output_chanel):\n",
    "        super(ProjBlock,self).__init__()\n",
    "        self.input_chanel=input_chanel\n",
    "        self.output_chanel=output_chanel\n",
    "        \n",
    "        self.conv1=nn.Conv2d(input_chanel,output_chanel,kernel_size=3,padding=1) \n",
    "        self.act1=nn.Tanh()\n",
    "        self.pool1=nn.MaxPool2d(2)\n",
    "        \n",
    "      #  self.conv2=nn.Conv2d(2*output_chanel,output_chanel,kernel_size=3,padding=1) \n",
    "      #  self.act1=nn.Tanh()\n",
    "      #  self.pool1=nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "      #  out = self.pool2(self.act2(self.conv2(x)))\n",
    "        return out\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self,num_chanel):\n",
    "        super(BasicBlock,self).__init__()\n",
    "        self.input_chanel=num_chanel\n",
    "        self.output_chanel=num_chanel\n",
    "        \n",
    "        self.conv=nn.Conv2d(self.input_chanel,self.output_chanel,kernel_size=3,padding=1)\n",
    "        self.act=nn.Tanh()\n",
    "        ## there should not be any MaxPooling layer in the inbetween set\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.act(self.conv(x))\n",
    "        return out\n",
    "\n",
    "# One is responsible for figuring out the unflatten shape\n",
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self,unflatten_shape): \n",
    "        super(FullyConnected,self).__init__()\n",
    "        self.unflatten_shape=unflatten_shape\n",
    "        self.fc1=nn.Linear(unflatten_shape,32)\n",
    "        self.ac1=nn.Tanh()\n",
    "        self.fc2=nn.Linear(32,10) \n",
    "        # Let's only tell the airplane from a bird\n",
    "    \n",
    "    def forward(self,x):\n",
    "        inputx=x.view(-1, self.unflatten_shape)\n",
    "        out=self.fc2(self.ac1(self.fc1(inputx)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a393e0",
   "metadata": {},
   "source": [
    "### Structure of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4c5b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()\n",
    "class ForwardModel(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(ForwardModel,self).__init__()\n",
    "        \n",
    "        self.config=config\n",
    "        self.batch_size=self.config.batch_size\n",
    "        self.Ntime=self.config.Ntime\n",
    "        self.sqrt_deltaT=self.config.sqrt_deltaT;\n",
    "        self.n_layer=self.config.n_layer\n",
    "        self.delta=self.config.totalT/self.Ntime;\n",
    "        \n",
    "        ## The structure is merely a stack-up of the convolutional blocks\n",
    "        self.mList=nn.ModuleList([ProjBlock(self.config.input_chanel,self.config.output_chanel_pj1),\n",
    "                                  ProjBlock(self.config.output_chanel_pj1,self.config.output_chanel_pj2),\n",
    "                                  BasicBlock(self.config.output_chanel_pj2),\n",
    "                                  BasicBlock(self.config.output_chanel_pj2),\n",
    "                                  BasicBlock(self.config.output_chanel_pj2),\n",
    "                                  BasicBlock(self.config.output_chanel_pj2),\n",
    "                                  FullyConnected(self.config.unflatten_shape)                              \n",
    "        ])\n",
    "        \n",
    "        self.sigma=0.25\n",
    "        \n",
    "    def forwardX(self,x):# here x is the batch collection of images\n",
    "        \n",
    "        # Constructing the noises\n",
    "        # The number 8 is determined from the number of max-pooling size, kernels & paddings etc. \n",
    "        xMat=[]\n",
    "        wMat=torch.FloatTensor(normal.rvs(size=[self.batch_size,\n",
    "                                     self.config.output_chanel_pj2,7,7,\n",
    "                                     self.Ntime]) * self.sqrt_deltaT).to(device)\n",
    "        x0=torch.clone(x).to(device); \n",
    "        xMat.append(x0); \n",
    "        \n",
    "        x_pj1=self.mList[0](x0); \n",
    "        xMat.append(x_pj1.to(device)); \n",
    "        x_input=self.mList[1](x_pj1)\n",
    "        xMat.append(x_input.to(device));\n",
    "        \n",
    "        for i in range(self.Ntime):\n",
    "            # i + 2 because we already have two layers before\n",
    "            xtemp=xMat[i+2]+self.mList[i+2](xMat[i+2])*self.delta +self.sigma*wMat[:,:,:,:,i] \n",
    "            xMat.append(xtemp.to(device))\n",
    "        \n",
    "        x_terminal=self.mList[-1](xMat[-1])\n",
    "        xMat.append(x_terminal.to(device))\n",
    "        \n",
    "        \n",
    "        return xMat, wMat\n",
    "        \n",
    "        # The input of the target must be a tensor not a list\n",
    "    def backwardYZ(self,xMat,wMat,target):\n",
    "        yMat=[];  \n",
    "        \n",
    "        L=len(xMat)\n",
    "        x_terminal=xMat[-1].to(device)\n",
    "        \n",
    "        loss_val=loss_fn(x_terminal,target.to(device))\n",
    "        loss_val.to(device); \n",
    "        \n",
    "        y_terminal=torch.autograd.grad(outputs=[loss_val], inputs=[x_terminal], grad_outputs=torch.ones_like(loss_val), allow_unused=True,\n",
    "                                 retain_graph=True, create_graph=True)[0]\n",
    "        #Here y_terminal has dim batch_size x output_size (2 x 2)\n",
    "        yMat.append(y_terminal.to(device)); \n",
    "        xtemp=xMat[L-2].to(device) # 3 \n",
    "        \n",
    "        ## Finding Y[T-1]\n",
    "        hami=torch.sum(y_terminal.detach()*self.mList[-1](xtemp),dim=1,keepdim=True) # keep dim=1 is correct\n",
    "        hami=hami.view(-1,1);hami.to(device)\n",
    "\n",
    "        hami_x=torch.autograd.grad(outputs=[hami], inputs=[xtemp], grad_outputs=torch.ones_like(hami),allow_unused=True,\n",
    "                                 retain_graph=True, create_graph=True)[0]\n",
    "    \n",
    "        yMat.append(hami_x.to(device))\n",
    "        \n",
    "        for i in range(self.Ntime-1,-1,-1):\n",
    "            X=xMat[i+2].to(device); \n",
    "            hami=torch.sum(yMat[-1].detach()*self.mList[i+2](X),dim=(1,2,3))\n",
    "            hami=hami.view(-1,1); hami.to(device); \n",
    "            \n",
    "            hami_x=torch.autograd.grad(outputs=[hami], inputs=[X], grad_outputs=torch.ones_like(hami),allow_unused=True,\n",
    "                                 retain_graph=True, create_graph=True)[0]\n",
    "            ytemp=yMat[-1]+hami_x*self.delta\n",
    "\n",
    "            yMat.append(ytemp.to(device))\n",
    "       \n",
    "    ### Second projection layer\n",
    "        X=xMat[1].to(device); \n",
    "       # X.requires_grad\n",
    "        hami=torch.sum(yMat[-1].detach()*self.mList[1](X),dim=(1,2,3))\n",
    "        hami=hami.view(-1,1); hami.to(device); \n",
    "            \n",
    "        ytemp=torch.autograd.grad(outputs=[hami], inputs=[X], grad_outputs=torch.ones_like(hami),allow_unused=True,\n",
    "                                 retain_graph=True, create_graph=True)[0]\n",
    "        yMat.append(ytemp.to(device))\n",
    "        \n",
    "        X=xMat[0].to(device); \n",
    "        X.requires_grad=True\n",
    "        hami=torch.sum(yMat[-1].detach()*self.mList[0](X),dim=(1,2,3))\n",
    "        hami=hami.view(-1,1); hami.to(device)\n",
    "            \n",
    "        ytemp=torch.autograd.grad(outputs=[hami], inputs=[X], grad_outputs=torch.ones_like(hami),allow_unused=True,\n",
    "                                 retain_graph=True, create_graph=True)[0]\n",
    "        yMat.append(ytemp.to(device))\n",
    "        \n",
    "        return yMat  #yMat the order is reversed \n",
    "    \n",
    "    def HamCompute(self,xMat,yMat):\n",
    "        totalham=0.0\n",
    "        \n",
    "        for i in range(self.Ntime+3):\n",
    "            ham_temp=torch.sum(yMat[self.Ntime+2-i].detach()*self.mList[i](xMat[i].detach()) )  #inside the bracket =  +\\small_value * self.batch_size *self.mList[i]*self.mList[i] (No, this doesn't contain batchsize)\n",
    "            totalham+=ham_temp\n",
    "        \n",
    "        return totalham/self.batch_size/(self.Ntime+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26f69326",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"data/SNN_mnist_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96d572ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=ForwardModel(cfg)\n",
    "net.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc9cbc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(pretrained_model, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00cb9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "net.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3096b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM attack code\n",
    "# which also works in the batch case\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "#    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7422c",
   "metadata": {},
   "source": [
    "### Defining the test accuracy function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9fb4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_prediction(model,iters,imgs,labels):\n",
    "    temp=torch.zeros((cfg.batch_size,label_dim))\n",
    "    temp=temp.to(device)\n",
    "    for i in range(iters):\n",
    "        val,_=model.forwardX(imgs)\n",
    "        temp+=val[-1].to(device)\n",
    "    temp=temp/iters;\n",
    "    _,predicts=torch.max(temp,dim=1)\n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85e854f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,device, epsilon, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    correct_avg=0\n",
    "    \n",
    "    for imgs, labels in test_loader:\n",
    "        \n",
    "        imgs, labels=imgs.to(device), labels.to(device)\n",
    "        imgs.requires_grad=True\n",
    "        xm,wm=model.forwardX(imgs)\n",
    "        loss=loss_fn(xm[-1].to(device),labels)\n",
    "        _,predict_init=torch.max(xm[-1],dim=1)\n",
    "        \n",
    "        \n",
    "        model.zero_grad()\n",
    "   #     # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "        data_grad=imgs.grad.data\n",
    "        perturbed_data = fgsm_attack(imgs, epsilon, data_grad)\n",
    "        \n",
    "   #     output,_ = model.forwardX(perturbed_data)\n",
    "        \n",
    "   #     _,predict_final=torch.max(output[-1],dim=1)\n",
    "        \n",
    "        predict_final_avg=avg_prediction(net,10,perturbed_data,labels)\n",
    "        \n",
    "     #   correct += int((predict_final == labels.to(device)).sum())\n",
    "        correct_avg += int((predict_final_avg.to(device) == labels.to(device)).sum())\n",
    "        total += imgs.shape[0]\n",
    "    return  correct_avg/total # correct/total,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b92e8e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5281"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(net,device, 0.3, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25a044ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0,.05,.1,.15,0.2, 0.3,0.4,0.5] #.1, .15, .2, .25, .3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8106a264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9886\n",
      "0.9583\n",
      "0.9081\n",
      "0.8272\n",
      "0.7275\n",
      "0.5197\n",
      "0.3718\n",
      "0.2678\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc = test(net, device, eps, test_loader)\n",
    "    print(acc)\n",
    "    accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c21604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
