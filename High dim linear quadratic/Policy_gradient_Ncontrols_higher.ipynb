{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "badce957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time \n",
    "import logging\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from scipy.stats import multivariate_normal as normal\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8981738e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "data_type=torch.float32\n",
    "MOMENTUM = 0.99\n",
    "EPSILON = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eb9e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    n_layer = 4\n",
    "    batch_size = 1024\n",
    "    valid_size = 1024\n",
    "    \n",
    "    dim=100; \n",
    "    Ntime=10; \n",
    "    delta=0.1/Ntime\n",
    "    sqrt_deltaT=np.sqrt(0.1/Ntime); \n",
    "\n",
    "    logging_frequency = 100\n",
    "    verbose = True\n",
    "    y_init_range = [0, 1]\n",
    "    \n",
    "    num_hiddens = [dim,256,256,dim]\n",
    "    \n",
    "def get_config(name):\n",
    "    try:\n",
    "        return globals()[name]\n",
    "    except KeyError:\n",
    "        raise KeyError(\"config not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7b298f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg=get_config('Config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e0ed788",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function defines one block for the nn \n",
    "\"\"\"\n",
    "class Dense(nn.Module): \n",
    "    def __init__(self,cin, cout, batch_norm=False, activate=True): \n",
    "        super(Dense,self).__init__()\n",
    "        self.cin=cin; \n",
    "        self.cout=cout; \n",
    "        self.activate=activate; \n",
    "        \n",
    "        self.linear=nn.Linear(self.cin,self.cout) #The linear layer\n",
    "        #BatchNorm1d: it requires the input to be a correct size\n",
    "        if batch_norm: \n",
    "            self.bn=nn.BatchNorm1d(cout,eps=EPSILON,momentum=MOMENTUM)\n",
    "        else: \n",
    "            self.bn=None\n",
    "       # nn.init.normal_(self.linear.weight,std=5.0/np.sqrt(cin+cout))\n",
    "        # This is the He initialization\n",
    "        \n",
    "    def forward(self,x): \n",
    "        x=self.linear(x)\n",
    "        if self.bn is not None:\n",
    "            x=self.bn(x)\n",
    "        if self.activate:\n",
    "            x=torch.tanh(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f428b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Constructing the Policy control\n",
    "\n",
    "The control should take as input (X')=(t,X) as the input, and so the input dimension \n",
    "should be 1+dim\n",
    "\"\"\"\n",
    "class controlNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(controlNN,self).__init__()\n",
    "        self.config=config\n",
    "        \n",
    "        self.bn=nn.BatchNorm1d(config.num_hiddens[0],eps=EPSILON,momentum=MOMENTUM)\n",
    "        # range(1,5): 1,2,3,4\n",
    "        self.layers=[Dense(config.num_hiddens[i-1],config.num_hiddens[i]) for i in range(1, len(config.num_hiddens)-1)]\n",
    "        self.layers+=[Dense(config.num_hiddens[-2], config.num_hiddens[-1],activate=False)]\n",
    "        self.layers=nn.Sequential(*self.layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.bn(x)\n",
    "        x=self.layers(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5881dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.config=config\n",
    "        self.batch_size=self.config.batch_size\n",
    "        self.dim=self.config.dim\n",
    "        self.Ntime=self.config.Ntime\n",
    "        self.delta=self.config.delta\n",
    "        self.sqrt_deltaT =self.config.sqrt_deltaT\n",
    "        \n",
    "        ## We make the change here \n",
    "        self.mList=nn.ModuleList([controlNN(config) for _ in range(self.Ntime)])    # controlNN(self.config)\n",
    "        \n",
    "        self.time_stamp=torch.ones([self.batch_size,1, self.Ntime+1])*torch.arange(0,self.Ntime+1)*self.delta\n",
    "        \n",
    "        # x should have the size (batch_size,_)\n",
    "    def forwardX(self,x): \n",
    "        xMat=torch.zeros([self.batch_size, self.dim, self.Ntime+1])\n",
    "       # xcatMat=torch.zeros([self.batch_size, self.dim+1, self.Ntime+1])\n",
    "        \n",
    "        wMat=torch.FloatTensor(normal.rvs(size=[self.batch_size,self.dim,self.Ntime])*self.sqrt_deltaT)\n",
    "        wMat=torch.reshape(wMat,(self.batch_size,self.dim,self.Ntime)) # Reshaping is needed when dim==1 \n",
    "        \n",
    "        xinit=torch.clone(x)\n",
    "        xinit=torch.reshape(xinit,[self.batch_size, self.dim])\n",
    "        \n",
    "        xMat[:,:,0]=xinit \n",
    "        \n",
    "       # xcat=torch.cat((self.time_stamp[:,:,0], xMat[:,:,0]),1)\n",
    "        for i in range(0, self.Ntime):\n",
    "          #  xcat=torch.cat((self.time_stamp[:,:,i], xMat[:,:,i]),1)\n",
    "        #    xcatMat[:,:,i]= xcat\n",
    "\n",
    "            control_temp=self.mList[i](xMat[:,:,i]); \n",
    "\n",
    "            xMat[:,:,i+1]=xMat[:,:,i]+(-0.25*xMat[:,:,i]+control_temp)*self.delta \\\n",
    "            + (0.2*xMat[:,:,i]+control_temp)*wMat[:,:,i]  \n",
    "        \n",
    "        return xMat, wMat   \n",
    "    \n",
    "    def backwardYZ(self,xMat,wMat): \n",
    "        yMat=torch.zeros([self.batch_size, self.dim, self.Ntime+1]); \n",
    "        zMat=torch.zeros([self.batch_size, self.dim, self.Ntime]);\n",
    "        \n",
    "       # temp_sum=-torch.sum(xMat[:,:,-1],dim=1,keepdim=True); \n",
    "        yMat[:,:,-1]=-xMat[:,:,-1]    #temp_sum.repeat(1,self.dim)\n",
    "        \n",
    "        for i in range(self.Ntime-1, -1, -1):\n",
    "            zMat[:,:,i]= wMat[:,:,i]*yMat[:,:,i+1]/self.delta\n",
    "            # we will have to do the differentiation and everything\n",
    "            \n",
    "            X=xMat[:,:,i];\n",
    "          #  xcat=torch.cat((self.time_stamp[:,:,i],X),1)   \n",
    "            # y*b+Ïƒ*z+f\n",
    "            \n",
    "            ctrl=self.mList[i](X)\n",
    "            \n",
    "            hami=torch.sum(yMat[:,:,i+1].detach()*ctrl  \n",
    "                           +zMat[:,:,i].detach()*ctrl - ctrl*ctrl,\n",
    "                           dim=1, keepdim=True)\n",
    "            \n",
    "            hami_x=torch.autograd.grad(outputs=[hami], inputs=[X], grad_outputs=torch.ones_like(hami), allow_unused=True,\n",
    "                                 retain_graph=True, create_graph=True)[0]\n",
    "            \n",
    "            yMat[:,:,i]= yMat[:,:,i+1] + (-0.5*xMat[:,:,i]-0.25*yMat[:,:,i+1]+ 0.2*zMat[:,:,i]+hami_x)* self.delta \n",
    "            \n",
    "        return yMat, zMat\n",
    "    \n",
    "    \"\"\"\n",
    "    Here is the issue: even though we can get something that is relatively close. \n",
    "    The maximization in Hamiltonian requries that one does the the update for each time, and \n",
    "    so for each time block, the controls are independent. \n",
    "    However, here we choose to update all the control all together by using the parameters \n",
    "    in the net control. \n",
    "    \n",
    "    This should be prevented. \n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    def Hamcompute(self,xMat,yMat,zMat):\n",
    "        ham=0.0\n",
    "        for i in range(0,self.Ntime):\n",
    "         #   xcat=torch.cat((self.time_stamp[:,:,i],xMat[:,:,i]),1); \n",
    "            X=xMat[:,:,i]\n",
    "            temp=self.mList[i](X.detach()); \n",
    "    \n",
    "            ham+=torch.mean(torch.sum(yMat[:,:,i].detach()* temp +zMat[:,:,i].detach()*temp-temp*temp, dim=1, keepdim=True))  \n",
    "        return -ham\n",
    "    \n",
    "    def computeLoss(self,xMat):\n",
    "        loss=0.0 \n",
    "        for i in range(0,self.Ntime):\n",
    "            #xcat=torch.cat((self.time_stamp[:,:,i],xMat[:,:,i]),1)\n",
    "            X=xMat[:,:,i]\n",
    "            tempctrl=self.mList[i](X)\n",
    "            \n",
    "            loss+=torch.sum(0.25*torch.square(X)*self.delta + torch.square(tempctrl),dim=1,keepdim=True)  \n",
    "        \n",
    "        #0.5*torch.square(xMat[:,:,-1])\n",
    "        \n",
    "        sum_temp=torch.sum(xMat[:,:,-1], dim=1, keepdim=True)\n",
    "        \n",
    "        return torch.mean(loss+0.5*torch.square(sum_temp))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39985db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "    \n",
    "    model=NeuralNet(cfg);\n",
    "    x0=torch.ones([cfg.batch_size,cfg.dim])\n",
    "    \n",
    "    epoch=1000; \n",
    "    \n",
    "    optimizer=optim.Adam(model.parameters(),lr=1e-4)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[500], gamma=0.2) \n",
    "    \n",
    "    \n",
    "    for i in range(0, epoch):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        xmat,wmat=model.forwardX(x0)\n",
    "        ymat,zmat=model.backwardYZ(xmat,wmat)\n",
    "        # compute the Hamiltonian\n",
    "        ham=model.Hamcompute(xmat,ymat,zmat)\n",
    "        \n",
    "        ham.backward()\n",
    "       # torch.nn.utils.clip_grad_norm_(model.parameters(),6)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 ==0:           \n",
    "            print(\"Iter:\", i, torch.mean(ymat[:,:,0]).item(), model.computeLoss(xmat).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5006e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 -1.004112720489502 4778.1923828125\n",
      "Iter: 10 -1.0025595426559448 4767.36279296875\n",
      "Iter: 20 -1.0002959966659546 4750.10546875\n",
      "Iter: 30 -1.0003409385681152 4746.36962890625\n",
      "Iter: 40 -0.9981225728988647 4732.51904296875\n",
      "Iter: 50 -0.9966267943382263 4721.23095703125\n",
      "Iter: 60 -0.9936010837554932 4706.5830078125\n",
      "Iter: 70 -0.993013858795166 4692.7685546875\n",
      "Iter: 80 -0.9916049242019653 4684.3193359375\n",
      "Iter: 90 -0.9886389374732971 4673.2021484375\n",
      "Iter: 100 -0.9861000776290894 4658.08544921875\n",
      "Iter: 110 -0.9836843609809875 4643.08349609375\n",
      "Iter: 120 -0.9817636609077454 4634.248046875\n",
      "Iter: 130 -0.9785168170928955 4621.12158203125\n",
      "Iter: 140 -0.9766543507575989 4609.80859375\n",
      "Iter: 150 -0.9736326336860657 4601.67724609375\n",
      "Iter: 160 -0.9715568423271179 4590.04736328125\n",
      "Iter: 170 -0.9695888757705688 4581.1533203125\n",
      "Iter: 180 -0.9677788019180298 4576.05615234375\n",
      "Iter: 190 -0.965873122215271 4571.2783203125\n",
      "Iter: 200 -0.9647007584571838 4567.2333984375\n",
      "Iter: 210 -0.9631655812263489 4560.849609375\n",
      "Iter: 220 -0.962522566318512 4559.17724609375\n",
      "Iter: 230 -0.9617552161216736 4556.1220703125\n",
      "Iter: 240 -0.9610138535499573 4556.708984375\n",
      "Iter: 250 -0.9607044458389282 4554.19384765625\n",
      "Iter: 260 -0.9600990414619446 4553.5693359375\n",
      "Iter: 270 -0.9598400592803955 4550.50048828125\n",
      "Iter: 280 -0.9596847295761108 4551.9365234375\n",
      "Iter: 290 -0.9594170451164246 4552.970703125\n",
      "Iter: 300 -0.9592984914779663 4549.95068359375\n",
      "Iter: 310 -0.9593470692634583 4550.6650390625\n",
      "Iter: 320 -0.9589927792549133 4551.431640625\n",
      "Iter: 330 -0.9592369198799133 4552.5322265625\n",
      "Iter: 340 -0.9590721130371094 4549.384765625\n",
      "Iter: 350 -0.9591260552406311 4549.2451171875\n",
      "Iter: 360 -0.9590004682540894 4550.498046875\n",
      "Iter: 370 -0.9588172435760498 4546.00537109375\n",
      "Iter: 380 -0.9589492082595825 4552.494140625\n",
      "Iter: 390 -0.9587833881378174 4544.97412109375\n",
      "Iter: 400 -0.9588332176208496 4544.95947265625\n",
      "Iter: 410 -0.9590067267417908 4549.09765625\n",
      "Iter: 420 -0.9589264392852783 4548.67578125\n",
      "Iter: 430 -0.9590731859207153 4547.61328125\n",
      "Iter: 440 -0.9588743448257446 4549.708984375\n",
      "Iter: 450 -0.9587971568107605 4547.115234375\n",
      "Iter: 460 -0.9588162302970886 4547.83154296875\n",
      "Iter: 470 -0.9588029384613037 4546.234375\n",
      "Iter: 480 -0.9589053988456726 4548.41748046875\n",
      "Iter: 490 -0.9587887525558472 4546.41259765625\n",
      "Iter: 500 -0.9588891863822937 4549.43017578125\n",
      "Iter: 510 -0.9587613940238953 4545.32373046875\n",
      "Iter: 520 -0.9590099453926086 4550.36865234375\n",
      "Iter: 530 -0.9589393734931946 4547.6767578125\n",
      "Iter: 540 -0.9590293765068054 4548.8740234375\n",
      "Iter: 550 -0.9588680267333984 4547.630859375\n",
      "Iter: 560 -0.9587912559509277 4548.32275390625\n",
      "Iter: 570 -0.9588524699211121 4544.44775390625\n",
      "Iter: 580 -0.9588118195533752 4550.556640625\n",
      "Iter: 590 -0.9588022828102112 4545.67138671875\n",
      "Iter: 600 -0.9588475227355957 4549.3642578125\n",
      "Iter: 610 -0.9588808417320251 4547.88134765625\n",
      "Iter: 620 -0.9589225649833679 4547.5478515625\n",
      "Iter: 630 -0.9587270617485046 4546.7998046875\n",
      "Iter: 640 -0.9588270783424377 4545.92138671875\n",
      "Iter: 650 -0.9587181210517883 4545.17626953125\n",
      "Iter: 660 -0.9589405655860901 4547.5068359375\n",
      "Iter: 670 -0.9587737321853638 4543.84765625\n",
      "Iter: 680 -0.9587919116020203 4547.9443359375\n",
      "Iter: 690 -0.9588220715522766 4545.69384765625\n",
      "Iter: 700 -0.9588630199432373 4548.03466796875\n",
      "Iter: 710 -0.9589016437530518 4548.29052734375\n",
      "Iter: 720 -0.9588284492492676 4546.57568359375\n",
      "Iter: 730 -0.9588966965675354 4546.39892578125\n",
      "Iter: 740 -0.9588391184806824 4544.982421875\n",
      "Iter: 750 -0.9588456749916077 4547.39501953125\n"
     ]
    }
   ],
   "source": [
    "train(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc732943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813abb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4898a43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2175c01a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31518c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
